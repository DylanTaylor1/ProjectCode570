{
  "_name_or_path": "gpt2-medium",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "benepar": {
    "char_vocab": null,
    "hparams": {
      "anneal_rate": 2e-05,
      "attention_dropout": 0.2,
      "back_cycle": false,
      "back_layers": 3,
      "back_loss_constant": 1.0,
      "back_loss_type": "kl",
      "back_use_gold_trees": true,
      "batch_size": 64,
      "bpe_dropout": 0.0,
      "char_lstm_input_dropout": 0.2,
      "checks_per_epoch": 3,
      "clip_grad_norm": 0.0,
      "d_char_emb": 64,
      "d_ff": 2048,
      "d_kv": 64,
      "d_label_hidden": 256,
      "d_model": 1024,
      "d_tag_hidden": 256,
      "discrete_cats": 256,
      "encoder_gum": false,
      "encoder_max_len": 512,
      "force_root_constituent": false,
      "learning_rate": 3e-05,
      "learning_rate_warmup_steps": 160,
      "mask": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "max_consecutive_decays": 3,
      "max_len_dev": 0,
      "max_len_train": 0,
      "morpho_emb_dropout": 0.2,
      "novel_learning_rate": 0.0001,
      "num_heads": 8,
      "num_layers": 8,
      "predict_tags": false,
      "pretrained_divide": 20.0,
      "pretrained_model": "gpt2-medium",
      "relu_dropout": 0.1,
      "residual_dropout": 0.2,
      "step_decay_factor": 0.5,
      "step_decay_patience": 5,
      "tag_combine_interval": 300,
      "tag_combine_mask_thres": 0.05,
      "tag_combine_start": Infinity,
      "tag_loss_scale": 5.0,
      "tag_split_thres": 1.001,
      "tau": 3.0,
      "tau_min": 0.05,
      "tree_transform": null,
      "two_label": false,
      "two_label_subspan": false,
      "use_chars_lstm": false,
      "use_encoder": true,
      "use_forced_lm": false,
      "use_pretrained": true,
      "use_vq": true,
      "vq_commitment": 0.1,
      "vq_coreset_size_multiplier": 10,
      "vq_decay": 0.97,
      "vq_interpolate_steps": 1245,
      "vq_observe_steps": 1245,
      "vq_wait_steps": 1245
    },
    "label_vocab": {
      "": 0,
      "ADJP": 1,
      "ADJP::ADJP": 2,
      "ADJP::ADVP": 3,
      "ADJP::NP": 4,
      "ADJP::QP": 5,
      "ADVP": 6,
      "ADVP::ADJP": 7,
      "ADVP::ADVP": 8,
      "ADVP::PRT": 9,
      "CONJP": 10,
      "FRAG": 11,
      "FRAG::ADJP": 12,
      "FRAG::ADVP": 13,
      "FRAG::INTJ": 14,
      "FRAG::NP": 15,
      "FRAG::PP": 16,
      "FRAG::S": 17,
      "FRAG::S::ADJP": 18,
      "FRAG::S::VP": 19,
      "FRAG::SBAR": 20,
      "FRAG::SBARQ": 21,
      "FRAG::UCP": 22,
      "FRAG::VP": 23,
      "FRAG::WHADVP": 24,
      "FRAG::WHNP": 25,
      "FRAG::WHPP": 26,
      "INTJ": 27,
      "INTJ::S": 28,
      "LST": 29,
      "NAC": 30,
      "NP": 31,
      "NP::ADJP": 32,
      "NP::ADVP": 33,
      "NP::FRAG": 34,
      "NP::INTJ": 35,
      "NP::NP": 36,
      "NP::NP::NP": 37,
      "NP::NP::QP": 38,
      "NP::PP": 39,
      "NP::PRN": 40,
      "NP::QP": 41,
      "NP::S": 42,
      "NP::S::VP": 43,
      "NP::SBAR": 44,
      "NP::SBAR::S::VP": 45,
      "NX": 46,
      "NX::NX": 47,
      "NX::QP": 48,
      "NX::S": 49,
      "NX::S::VP": 50,
      "PP": 51,
      "PP::NP": 52,
      "PP::PP": 53,
      "PRN": 54,
      "PRN::FRAG::WHADJP": 55,
      "PRN::NP": 56,
      "PRN::PP": 57,
      "PRN::S": 58,
      "PRN::S::VP": 59,
      "PRN::SBAR": 60,
      "PRN::SINV": 61,
      "PRT": 62,
      "QP": 63,
      "RRC": 64,
      "RRC::VP": 65,
      "S": 66,
      "S::ADJP": 67,
      "S::ADVP": 68,
      "S::NP": 69,
      "S::PP": 70,
      "S::S": 71,
      "S::UCP": 72,
      "S::VP": 73,
      "S::VP::ADVP": 74,
      "S::VP::VP": 75,
      "SBAR": 76,
      "SBAR::FRAG": 77,
      "SBAR::S": 78,
      "SBAR::S::VP": 79,
      "SBAR::SBAR::S": 80,
      "SBAR::SBARQ": 81,
      "SBAR::SINV": 82,
      "SBAR::WHADVP": 83,
      "SBAR::WHNP": 84,
      "SBARQ": 85,
      "SBARQ::WHADVP": 86,
      "SINV": 87,
      "SQ": 88,
      "SQ::VP": 89,
      "UCP": 90,
      "UCP::ADJP": 91,
      "UCP::PP": 92,
      "VP": 93,
      "VP::ADVP": 94,
      "VP::FRAG::ADJP": 95,
      "VP::NP": 96,
      "VP::PP": 97,
      "VP::S::VP": 98,
      "VP::SBAR": 99,
      "VP::VP": 100,
      "WHADJP": 101,
      "WHADVP": 102,
      "WHNP": 103,
      "WHNP::QP": 104,
      "WHNP::WHNP": 105,
      "WHPP": 106,
      "X": 107,
      "X::ADVP": 108,
      "X::NP": 109,
      "X::PP": 110,
      "X::SBARQ": 111,
      "X::VP": 112
    },
    "tag_vocab": {
      "#": 1,
      "$": 2,
      "''": 3,
      ",": 4,
      "-LRB-": 5,
      "-RRB-": 6,
      ".": 7,
      ":": 8,
      "CC": 9,
      "CD": 10,
      "DT": 11,
      "EX": 12,
      "FW": 13,
      "IN": 14,
      "JJ": 15,
      "JJR": 16,
      "JJS": 17,
      "LS": 18,
      "MD": 19,
      "NN": 20,
      "NNP": 21,
      "NNPS": 22,
      "NNS": 23,
      "PDT": 24,
      "POS": 25,
      "PRP": 26,
      "PRP$": 27,
      "RB": 28,
      "RBR": 29,
      "RBS": 30,
      "RP": 31,
      "SYM": 32,
      "TO": 33,
      "UH": 34,
      "UNK": 0,
      "VB": 35,
      "VBD": 36,
      "VBG": 37,
      "VBN": 38,
      "VBP": 39,
      "VBZ": 40,
      "WDT": 41,
      "WP": 42,
      "WP$": 43,
      "WRB": 44,
      "``": 45
    }
  },
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.5.1",
  "use_cache": true,
  "vocab_size": 50257
}
